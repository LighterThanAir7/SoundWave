Would you like to implement any additional features like:

    Token refresh mechanism
    Remember me functionality
    Secure logout endpoint
    Session timeout handling


Regarding the naming convention, _on is indeed more intuitive for datetime fields as it indicates "when" something happened, while _at is more commonly used for timestamps in some frameworks. For artwork handling, storing the file path is definitely better than base64 in the database. You could:

    Extract artwork from MP3
    Save it as a separate file
    Store only the path in the database
    Use the embedded artwork as fallback if file is missing

-- For likes/favorites
CREATE TABLE user_song_interactions (
    id int(10) NOT NULL AUTO_INCREMENT PRIMARY KEY,
    user_id int(10) NOT NULL,
    song_id int(10) NOT NULL,
    is_favorite boolean DEFAULT false,
    play_count int DEFAULT 0,
    last_played timestamp,
    created_on timestamp DEFAULT CURRENT_TIMESTAMP,
    UNIQUE KEY unique_user_song (user_id, song_id),
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (song_id) REFERENCES songs(id)
);

-- For song lyrics (optional)
CREATE TABLE lyrics (
    id int(10) NOT NULL AUTO_INCREMENT PRIMARY KEY,
    song_id int(10) NOT NULL UNIQUE,
    text text NOT NULL,
    language varchar(5) DEFAULT 'en',
    created_on timestamp DEFAULT CURRENT_TIMESTAMP,
    updated_on timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (song_id) REFERENCES songs(id)
);
-- For play history
CREATE TABLE play_history (
    id int(10) NOT NULL AUTO_INCREMENT PRIMARY KEY,
    user_id int(10) NOT NULL,
    song_id int(10) NOT NULL,
    played_at timestamp DEFAULT CURRENT_TIMESTAMP,
    played_duration int, -- in seconds, to track if song was played partially
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (song_id) REFERENCES songs(id),
    INDEX idx_user_played (user_id, played_at)
);

-- For user activity logging
CREATE TABLE user_activity (
    id int(10) NOT NULL AUTO_INCREMENT PRIMARY KEY,
    user_id int(10) NOT NULL,
    activity_type ENUM('PLAY', 'LIKE', 'CREATE_PLAYLIST', 'ADD_TO_PLAYLIST', 'FOLLOW_ARTIST', 'RATE'),
    entity_type ENUM('SONG', 'PLAYLIST', 'ARTIST', 'ALBUM'),
    entity_id int(10) NOT NULL,
    created_on timestamp DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id),
    INDEX idx_user_activity (user_id, created_on)
);

-- For ratings
CREATE TABLE ratings (
    id int(10) NOT NULL AUTO_INCREMENT PRIMARY KEY,
    user_id int(10) NOT NULL,
    entity_type ENUM('SONG', 'ALBUM', 'PLAYLIST'),
    entity_id int(10) NOT NULL,
    rating tinyint NOT NULL CHECK (rating BETWEEN 1 AND 5),
    created_on timestamp DEFAULT CURRENT_TIMESTAMP,
    updated_on timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    UNIQUE KEY unique_user_rating (user_id, entity_type, entity_id),
    FOREIGN KEY (user_id) REFERENCES users(id),
    INDEX idx_entity (entity_type, entity_id)
);

Based on the search results, detecting duplicate audio files requires sophisticated audio fingerprinting that can identify similar songs even with differences in:

    Audio quality and bitrate1
    File length (slightly shorter or longer versions)1
    Volume levels and audio filtering1
    File formats (MP3, AAC, FLAC, etc.)1
    3
    Metadata/ID3 tags3

For your implementation, you could:

    Use audio fingerprinting algorithms to analyze the actual audio content rather than just metadata4
    Compare audio files based on sound content rather than binary patterns6
    Set similarity thresholds to control how strict the matching should be1

Matching Criteria

    Duration differences
    Bitrate variations
    Volume differences
    Audio filtering and noise
    File format conversions

This would help identify duplicates even when:

    Files have different names
    Songs are in different formats
    Audio quality varies
    Length slightly differs
    Volume levels are different

Would you like me to show how to implement audio fingerprinting in your current upload system?


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


    Add progress tracking for large uploads:

javascript
const handleBatchUpload = async (event) => {
    event.preventDefault();
    if (!selectedBatchFiles) return;

    const files = Array.from(selectedBatchFiles).filter(
        file => file.type === 'audio/mpeg'
    );

    const formData = new FormData();
    files.forEach(file => formData.append('files', file));

    setUploading(true);

    try {
        const response = await fetch(`${API_URL}/api/upload/songs/batch`, {
            method: 'POST',
            body: formData,
            onUploadProgress: (progressEvent) => {
                const percentCompleted = Math.round(
                    (progressEvent.loaded * 100) / progressEvent.total
                );
                setProgress(percentCompleted);
            }
        });

        // ... rest of the code
    }
};

    Add server-side chunking and progress updates:

javascript
export const handleBatchSongUpload = async (req, res) => {
    try {
        const totalFiles = req.files.length;
        const processedFiles = [];
        const errors = [];
        let processed = 0;

        for (const file of req.files) {
            try {
                const tags = NodeID3.read(file.path);
                processedFiles.push({
                    filename: file.filename,
                    path: file.path,
                    metadata: tags
                });

                // Update progress every 100 files
                processed++;
                if (processed % 100 === 0) {
                    // If using WebSocket or Server-Sent Events
                    // emit progress update
                    console.log(`Processed ${processed}/${totalFiles} files`);
                }
            } catch (error) {
                errors.push({
                    filename: file.originalname,
                    error: error.message
                });
            }
        }

        res.json({
            message: 'Batch upload processed',
            totalFiles,
            successfulUploads: processedFiles.length,
            failedUploads: errors.length,
            processedFiles,
            errors
        });

    } catch (error) {
        console.error('Batch upload error:', error);
        res.status(500).json({
            message: 'Error processing batch upload',
            error: error.message
        });
    }
};

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Recommendations:

    Set limit to 5000 files initially (can be adjusted based on testing)
    Add proper progress tracking
    Implement chunked processing
    Add server-side progress updates
    Consider implementing resume capability for failed uploads
    Add proper error handling for timeouts
    Consider implementing WebSocket or SSE for real-time progress updates
